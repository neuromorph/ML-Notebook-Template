{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =Title - ML Problem="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author @contact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem statement. Source of the problem and/or data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why the problem is interesting/challenging - motivation/inspiration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current SoTA, related work with references.\n",
    "Your results based on this notebook.\n",
    "Remarks on the methods, results or analysis done here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup scripts to run if any.\n",
    "All the library imports.\n",
    "\n",
    "Create a git project. Connect remote to github or own git server. Pipeline to push notebook updates to server.\n",
    "\n",
    "Setup/Connect experiment tracking and visualization tools - Tensorboard, W&B etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document known information about the data - likely from the dataset source. e.g. type of data, how many data samples, train/test split, data format, dataset size etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Stats\n",
    "Download and extract data.\n",
    "List the dataset dir contents.\n",
    "Get stats on data and ensure it matches to any known information above.\n",
    "Are data classes balanced - do you need any oversampling? (May not be an issue if the lowest data samples class also has resonable number of samples for the problem at hand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "Load the data as training and validations sets. May need to split data and/or extract labels from folder/file names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "List head of files to check content.\n",
    "If images, plot samples from all the classes.\n",
    "Get an idea on how clean the data is - image quality, centered or not, possibility of issues due to transformations that you might do, contrast/color, missing values, null/nan values, categorical data, date fields, etc\n",
    "List any potenial issues you notice so they can be handled later. Another round of this can happen during top losses analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformations\n",
    "Note: Identify and perform only essential transofrmations initially for subset analysis and initial exploration \n",
    "<br>Note: Types of transformations - 1. transformation for augmenting data, 2. transformation to prepare data (e.g. resize image, hist equalize, audio to spectogram etc)\n",
    "<br><br>image - color, contrast, resize, crop, morphological operations, \n",
    "<br>tabular - numericalize, missing values, date fields, \n",
    "<br>Visualize, verify transformed data samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataset contains too many data samples then exploratory training can be very expensive and it would be a good idea to work on a subset first.\n",
    "Select a subset that is likely to be representative as much possible. \n",
    "e.g. consider only few of the classes, few random samples - specifically include important but rare samples if any, reduce image size, \n",
    "\n",
    "use simpler / smaller model first as baseline\n",
    "sanity checks\n",
    "overfit a batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Select suitable model based on previous baseline exploration and also check what models have been successfully applied for similar tasks. For a lot of problems, there should be some existing model that can be applied directly or with very minor modification. An important motivation is also to be able to use a pretrained model. Thus, check for available pretrained models in that domain to be leveraged for task at hand. \n",
    "\n",
    "- Check for suitable pretrained model that can be leveraged for this task\n",
    "- Select an existing and proven model architecture for the field (whether pretrained or not) over new SOTA published yesterday or coming up with brand new\n",
    "- Apply any minor modifications needed, e.g. last/head layer(s) for pretrained model\n",
    "- Decide on data breakup to create train/validation/test sets\n",
    "- Decide on important relevant metrics to track for the task\n",
    "- Decide on suitable loss function\n",
    "- Note the hyperparameters involved \n",
    "- Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleanup\n",
    "Mislabeled data samples in training to be fixed. Check samples corresponding to top losses to find mislabled or check data acquisation/curation process. Small % of mislabled maybe OK depending on algorithm. On the other hand large % of mislabled can be expensive to fix.\n",
    "<br>Invalid images - size zero, wrong image, not as expected during testing to be removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "Loss curves, confusion matrix, top losses, overfitting/underfitting\n",
    "Error analysis and debugging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to try\n",
    "- Data: get more data , preprocessing, augmentations, remove leakage\n",
    "- Architecture - verify\n",
    "- Progressive resizing of images\n",
    "- Test time augmentations\n",
    "- Mixed precision floating point operations (quantization)\n",
    "- Check initializations, normalizations\n",
    "- Weight decay for regularization, along with dropout\n",
    "- Learning Rates Scheduling and Different LR for different layer groups (when unfreezing pretrained)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
